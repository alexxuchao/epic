Context Note
Chosen Scenario & Micro-Task:
E-commerce automatic image generation for Enterprise customers requiring marketing-ready images at scale. The micro-task addresses systematic evaluation of AI-generated product campaign images across three critical dimensions: image quality, brand consistency, and source alignment. The prototype guides evaluators through a complete workflow (upload → AI evaluation → human review → export) that demonstrates how cognitively demanding manual labeling can be transformed into efficient proofreading at enterprise scale.
AI Coding Tool(s) & Approach:
Primary tool: Claude for end-to-end development workflow. Strategic approach: Started with PRD creation to drive accurate code generation, then generated evaluation dataset (quality-spectrum prompts + real source images + GPT-4o generation) as the dataset itself serves as a problem-understanding PRD in ML contexts. Built functional prototype with real GPT-4V integration plus mock fallback, demonstrating practical AI judge implementation. Iterative prompting focused on creating self-explanatory UI that transparently communicates evaluation methodology and AI reasoning to users without external documentation.
Target Evaluator Insight:
The key insight evaluators will gain is experiencing firsthand how structured AI evaluation with transparent reasoning can maintain consistency at scale while revealing where human judgment remains irreplaceable. By interacting with both AI assessments and human annotation interfaces, evaluators will understand the practical balance needed between automation (for cognitive load reduction) and human oversight (for nuanced brand/quality decisions) in enterprise creative AI workflows.
